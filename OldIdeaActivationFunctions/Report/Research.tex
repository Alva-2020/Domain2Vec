\documentclass[]{article}

% Useful packages

\usepackage{latexsym}
\usepackage{bm} % bold math -- good for bolded subscripts
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm,amsfonts}
\usepackage{url}
\usepackage{enumerate}
\usepackage[margin=1in]{geometry}

% Theorem type environments

\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem{cor}{Corollary}
\newtheorem{defn}{Definition}
\newtheorem{assump}{Assumption}

% Useful macros -- you can add your own

\newcommand{\sH}{{\mathcal H}} 
\newcommand{\sX}{{\mathcal X}} 
\newcommand{\sY}{{\mathcal Y}} 
\newcommand{\sR}{{\mathcal R}}
\newcommand{\hnhat}{\widehat{h}_n}
\newcommand{\nn}{\nonumber}

% frequently used symbols
\newcommand{\bE}{\mathbb{E}} % use this for expectation
\newcommand{\bR}{\mathbb{R}} % real numbers
\newcommand{\bQ}{\mathbb{Q}}
\newcommand{\bN}{\mathbb{N}}
\newcommand{\bZ}{\mathbb{Z}}

% operators
\newcommand{\sign}{\mathop{\mathrm{sign}}}
\newcommand{\supp}{\mathop{\mathrm{supp}}} % support
\newcommand{\argmin}{\operatornamewithlimits{arg\ min}}
\newcommand{\argmax}{\operatornamewithlimits{arg\ max}}

% indicator function
\newcommand{\ind}[1]{\bm{1}_{\{#1\}}} 

% grouping operators
\newcommand{\brac}[1]{\left[#1\right]}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\paren}[1]{\left(#1\right)}
\newcommand{\norm}[1]{\left\|#1\right\|}
\newcommand{\ip}[2]{\left\langle #1,#2 \right\rangle}

\begin{document}

\title{Kernel Learning and Neural Networks}
\author{}
\maketitle

\section{Main Idea}

In support vector machine we (try to find a classifier $ f: \phi(x) \rightarrow y $ ie. $f(x) = w^T\phi(x)  $ in this case) solve the following problem - 
\begin{equation}
 \arg \min_{w} \frac{\lambda}{2} w^Tw + \frac{1}{N} \sum_{i = 1}^N L(y_i, w^T\phi(x_i) ). 
 \end{equation}

In this case $ \phi(x)$ is the feature mapping of the kernel used. using the kernel approximation technique we can approximate this to $z(x)$. If we use family of shift invariant kernels and random Fourier features to approximate them, then $ z(x)$  is going to be a function of sine/cosine. So idea is to optimize over following - 

\begin{equation}
	\label{eq:proposed_opt}
 \arg \min_{\theta, w} \frac{\lambda}{2} w^Tw + \frac{1}{N} \sum_{i = 1}^N L(y_i, w^Tz_{\theta}(x_i) ). 
 \end{equation}

\section{Shift Invariant Kernels}
\subsection{Random Fourier Features}
Random Fourier features use sine/cosine as their basis function to approximate the kernel and take advantage of  Theorem~\ref{thm:bochner} to approximate the kernel
\begin{thm}
    \label{thm:bochner}
    A continuos kernel $k(x,y) = k(x-y)$ on $R^d$ is positive definite iff $k(\delta)$ is the Fourier transform of a non-negative measure.
\end{thm}

If a shift variant kernel $k(\delta)$ is properly scaled then Theorem~\ref{thm:bochner} guarantees that $p(\theta)$ in the  \ref{eq:bochner_eq} is a proper probability distribution. 
    \begin{equation}
        \label{eq:bochner_eq}
       k(x-y) = \int_{R^d} p(\theta)e^{j\theta^T(x-y)}d\theta = E_{\theta}(\zeta_{\theta}(x)\zeta_{\theta}(y)^*),
    \end{equation}

 \cite{rahimi2007random}. Basically random Fourier features approximate the integral in \ref{eq:bochner_eq} using samples drawn from $p(\theta)$. Let's say we draw L samples $\theta_1, \theta_2,..., \theta_L$ samples from $p(\theta)$. 
\begin{eqnarray*}
    \label{eq:rff}
    k(x-y)  &=& \int_{R^d} p(\theta)e^{j\theta^T(x-y)}d\theta  \\
    &=&   \int_{R^d} p(\theta)cos(\theta^Tx-\theta^Ty)d\theta   \\
   & \approx & \frac {1}{L} \sum_{i=1}^{L} cos(\theta_i^Tx-\theta_i^Ty)\\
 	&=& \frac {1}{L} \sum_{i=1}^{L} cos(\theta_i^Tx) cos(\theta_i^Ty) + sin(\theta_i^Tx) sin(\theta_i^Ty)\\
	&=& \frac {1}{L} \sum_{i=1}^{L} [cos(\theta_i^Tx) , sin(\theta_i^Tx)]^T [cos(\theta_i^Ty)  , sin(\theta_i^Ty)] \\
	&=& z(x)^Tz(y)
\end{eqnarray*}

where $z(x) =  \frac {1}{\sqrt L} [cos(\theta_i^Tx) , sin(\theta_i^Tx)] \in R^{2L}$ is an approximate non linear feature mapping (mapped to $2L$ dimensional space).  
%
%\begin{algorithm}
%\caption{Random Fourier Features - RFF}
%\label{alg:RFF_algo}
%\begin{algorithmic}[1]
%\State Input: A positive definte shift invariant kernel $k(x,y) = k(x-y)$
%\State Compute: Fourier transform $p$ of the kernel $k$: $p(w) = \frac{1}{2\pi} \int_{R^d} e^{-jw^T\delta}k(\delta)d\Delta$ 
%\State Draw L iid $w_1, w_2,..., w_L \in R^d$ samples from $p$ 
%\State Return: $z(x) =  \frac {1}{\sqrt L} [cos(w_i^Tx) , sin(w_i^Tx)]_{i=1}^L \in R^{2L}$ 
%\end{algorithmic}
%\end{algorithm}


\section{Neural Netoworks}


\section{Related papers to keep in mind}

\begin{itemize}
    \item 1) An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections
    \item 2) https://www.researchgate.net/publication/3835580_Neural_networks_with_periodic_and_monotonic_activation_functions_a_comparative_study_in_classification_problems
    \item https://link.springer.com/chapter/10.1007%2F978-3-642-20353-4_5
    \item  https://openreview.net/pdf?id=Sks3zF9eg - TAMING THE WAVES: SINE AS ACTIVATION FUNCTION IN DEEP NEURAL NETWORK
\end{itemize}



\section{Challenges}
\begin{itemize}
\item Learning $\theta $ in eqn~\ref{eq:proposed_opt} can be thought of as a single layer NN. What happens if we add multiple layers? Is it still a family of shift invariant kernels?
\item What if we use activation functions motivated by RFF - sine/cosine? What does that lead to? Is it an interesting activation function?
\item How does this activation function compared to current state-of-art activation functions? 
\item Does this help us in understanding NN fundamentally? Can we prove anything interesting or anything that leads to better intuition about NN?
\item Can we imitate the computation in deep NNs using an appropriate kernel ?
\item Assume kernel k1 corresponds to a particular neural net n1. Similarly, assume k2 corresponds to n2. We can make kernels by composing kernels in different ways. Can we make an analogy between such composition of kernels and composition of neural nets ?

\end{itemize}


\section{Timeline}
\begin{itemize}
\item Implement paper [2]
\item Implement NN and deep NN for eqn~\ref{eq:proposed_opt}. 
\item 
\item
\item
\end{itemize}



\begin{thebibliography}{1}
[1] Rahimi, Ali, and Benjamin Recht. "Random features for large-scale kernel machines." Advances in neural information processing systems. 2007. 
[2] Yu, Felix X., et al. "Compact Nonlinear Maps and Circulant Extensions." arXiv preprint arXiv:1503.03893 (2015).

\end{thebibliography}
\end{document}